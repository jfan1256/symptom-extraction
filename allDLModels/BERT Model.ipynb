{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4367002988886098852\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9148379955\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18149310753749780447\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(device_count = {'GPU': 6, 'CPU' : 49} )\n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "K.set_session(sess)\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>RECORD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>OC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>gallstone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>pancreatitis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence          Word  Tag\n",
       "0  Sentence: 1        RECORD    0\n",
       "1  Sentence: 2            OC    0\n",
       "2          NaN            AM    0\n",
       "3          NaN     gallstone    0\n",
       "4          NaN  pancreatitis    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./OUTPUTAnnotations/dataset.csv', encoding= 'unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp/ipykernel_4008/2003111703.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_fillna.groupby(['Sentence'],as_index=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[RECORD]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[WILL, D/C, ORDER, BE, USED, AS, THE, D/C, SUM...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[prandial, N/V/severe, upper, abdominal, pain....</td>\n",
       "      <td>[0, 1, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[normal, limits., Cardiac, catheterization, da...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[year, old, Black, female, with, significant, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sentence                                               Word  \\\n",
       "0      Sentence: 1                                           [RECORD]   \n",
       "1     Sentence: 10  [WILL, D/C, ORDER, BE, USED, AS, THE, D/C, SUM...   \n",
       "2    Sentence: 100  [prandial, N/V/severe, upper, abdominal, pain....   \n",
       "3   Sentence: 1000  [normal, limits., Cardiac, catheterization, da...   \n",
       "4  Sentence: 10000  [year, old, Black, female, with, significant, ...   \n",
       "\n",
       "                              Tag  \n",
       "0                             [0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2     [0, 1, 0, 1, 1, 0, 0, 0, 0]  \n",
       "3        [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "data_group = data_fillna.groupby(['Sentence'],as_index=False\n",
    "                                )['Word', 'Tag'].agg(lambda x: list(x))\n",
    "\n",
    "#data_fillna\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_group['Word'].tolist()  \n",
    "labels = data_group['Tag'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensAll = []\n",
    "tagsAll = []\n",
    "for tokenLine, tagLine in zip(texts, labels):\n",
    "    for token, tag in zip(tokenLine, tagLine):\n",
    "        tokensAll.append(token)\n",
    "        tagsAll.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n"
     ]
    }
   ],
   "source": [
    "indexSequence = 0\n",
    "tokens607 = []\n",
    "tags607 = []\n",
    "count = 0\n",
    "tokens = []\n",
    "tags = []\n",
    "firstTime = 0\n",
    "appended = 0\n",
    "while indexSequence < len(tokensAll):\n",
    "    if count != 512:\n",
    "        tokens.append(tokensAll[indexSequence])\n",
    "        tags.append(tagsAll[indexSequence])\n",
    "        indexSequence = indexSequence + 1\n",
    "        count = count + 1\n",
    "    elif count == 512 and firstTime == 0:\n",
    "        tokens607 = np.array(tokens)\n",
    "        tags607 = np.array(tags)\n",
    "        firstTime = 1\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        indexSequence = indexSequence-112\n",
    "        count = 0 \n",
    "        appended = 1\n",
    "        print(appended)\n",
    "    elif count == 512 and firstTime == 1:\n",
    "        tokens607 = np.vstack((tokens607, np.array(tokens)))\n",
    "        tags607 = np.vstack((tags607, np.array(tags)))\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        indexSequence = indexSequence-112\n",
    "        count = 0 \n",
    "        appended = appended + 1\n",
    "        print(appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stride</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stride: 1</td>\n",
       "      <td>[RECORD, WILL, D/C, ORDER, BE, USED, AS, THE, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stride: 2</td>\n",
       "      <td>[well, expanded., He, was, found, to, be, hypo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stride: 3</td>\n",
       "      <td>[EMSSten, Tel, Dictated, By:, QUARRY, FERNANDO...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stride: 4</td>\n",
       "      <td>[lymph, node., She, should, have, repeat, CT, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stride: 5</td>\n",
       "      <td>[room, air., The, patient, appeared, in, no, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>Stride: 2370</td>\n",
       "      <td>[weeks, of, increase, in, her, postnasal, drip...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>Stride: 2371</td>\n",
       "      <td>[The, urinalysis, revealed, too, numerous, to,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>Stride: 2372</td>\n",
       "      <td>[all., This, revealed, that, she, also, has, c...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>Stride: 2373</td>\n",
       "      <td>[signs:, Temperature, degrees, blood, pressure...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>Stride: 2374</td>\n",
       "      <td>[RECORD, which, concluded, that, he, was, bein...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2374 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Stride                                               Word  \\\n",
       "0        Stride: 1  [RECORD, WILL, D/C, ORDER, BE, USED, AS, THE, ...   \n",
       "1        Stride: 2  [well, expanded., He, was, found, to, be, hypo...   \n",
       "2        Stride: 3  [EMSSten, Tel, Dictated, By:, QUARRY, FERNANDO...   \n",
       "3        Stride: 4  [lymph, node., She, should, have, repeat, CT, ...   \n",
       "4        Stride: 5  [room, air., The, patient, appeared, in, no, a...   \n",
       "...            ...                                                ...   \n",
       "2369  Stride: 2370  [weeks, of, increase, in, her, postnasal, drip...   \n",
       "2370  Stride: 2371  [The, urinalysis, revealed, too, numerous, to,...   \n",
       "2371  Stride: 2372  [all., This, revealed, that, she, also, has, c...   \n",
       "2372  Stride: 2373  [signs:, Temperature, degrees, blood, pressure...   \n",
       "2373  Stride: 2374  [RECORD, which, concluded, that, he, was, bein...   \n",
       "\n",
       "                                                    Tag  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "2369  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "2370  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2371  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2372  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2373  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[2374 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataBert = pd.DataFrame(columns = ['Stride', 'Word', 'Tag'])\n",
    "stride = 1\n",
    "count = 0\n",
    "while count < len(tokens607):\n",
    "    dataBert.loc[len(dataBert.index)] = ['Stride: ' + str(stride), tokens607[count], tags607[count]] \n",
    "    count = count + 1\n",
    "    stride = stride + 1\n",
    "\n",
    "dataBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(dataBert, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset_train = Dataset.from_pandas(data_train)\n",
    "dataset_test = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8b8ac4e19d4e0998d4395ce24c4410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c64f18d3feb421ba8142c777a396f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"Word\"], truncation=True, is_split_into_words=True, padding = 'max_length', max_length =512)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"Tag\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                            # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:              # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81e466c292b48c0893b5255fb499676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db5b7b00b8e455e9cb06ff263af680a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_train = dataset_train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_data_test = dataset_test.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Stride': 'Stride: 607',\n",
       " 'Word': ['groin',\n",
       "  'on',\n",
       "  'was',\n",
       "  'negative',\n",
       "  'for',\n",
       "  'pseudoaneurysm.',\n",
       "  'Because',\n",
       "  'of',\n",
       "  'recurrent',\n",
       "  'stenosis',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'is',\n",
       "  'planning',\n",
       "  'to',\n",
       "  'be',\n",
       "  'transferred',\n",
       "  'to',\n",
       "  'Tia',\n",
       "  'Health',\n",
       "  'Of',\n",
       "  'in',\n",
       "  'Che',\n",
       "  'Fay',\n",
       "  'on',\n",
       "  'for',\n",
       "  'Robimycin',\n",
       "  'coated',\n",
       "  'stents.',\n",
       "  'Hematology:',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'maintained',\n",
       "  'on',\n",
       "  'intravenous',\n",
       "  'unfractionated',\n",
       "  'heparin',\n",
       "  'with',\n",
       "  'goal',\n",
       "  'PT',\n",
       "  'dialysis',\n",
       "  'days.',\n",
       "  'of',\n",
       "  'Her',\n",
       "  'PTT',\n",
       "  'on',\n",
       "  'the',\n",
       "  'day',\n",
       "  'of',\n",
       "  'discharge',\n",
       "  'was',\n",
       "  'in',\n",
       "  'therapeutic',\n",
       "  'range.',\n",
       "  'Renal:',\n",
       "  'The',\n",
       "  'renal',\n",
       "  'function',\n",
       "  'was',\n",
       "  'stable',\n",
       "  'post',\n",
       "  'catheterization.',\n",
       "  'CODE:',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'is',\n",
       "  'full',\n",
       "  'code.',\n",
       "  'DISPOSITION:',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'is',\n",
       "  'planned',\n",
       "  'to',\n",
       "  'undergo',\n",
       "  'transfer',\n",
       "  'to',\n",
       "  'Foote',\n",
       "  'Health',\n",
       "  'in',\n",
       "  'Mi',\n",
       "  'for',\n",
       "  'Robimycin',\n",
       "  'coated',\n",
       "  'stents.',\n",
       "  'The',\n",
       "  'transfer',\n",
       "  'will',\n",
       "  'take',\n",
       "  'place',\n",
       "  'by',\n",
       "  'ambulance',\n",
       "  'on',\n",
       "  'DISCHARGE',\n",
       "  'MEDICATIONS:',\n",
       "  'Unfractionated',\n",
       "  'heparin',\n",
       "  'at',\n",
       "  'units',\n",
       "  'per',\n",
       "  'hour',\n",
       "  'intravenous',\n",
       "  'TNG',\n",
       "  'at',\n",
       "  'mcg',\n",
       "  'per',\n",
       "  'minute',\n",
       "  'enteric',\n",
       "  'coated',\n",
       "  'aspirin',\n",
       "  'mg',\n",
       "  'q.d.',\n",
       "  'atenolol',\n",
       "  'mg',\n",
       "  'q.d.',\n",
       "  'Colace',\n",
       "  'mg',\n",
       "  'b.i.d.',\n",
       "  'Zestril',\n",
       "  'mg',\n",
       "  'q.d.',\n",
       "  'Zocor',\n",
       "  'mg',\n",
       "  'p.o.',\n",
       "  'q.h.s.',\n",
       "  'Plavix',\n",
       "  'mg',\n",
       "  'q.d.',\n",
       "  'Serax',\n",
       "  'mg',\n",
       "  'p.o.',\n",
       "  'q.h.s.',\n",
       "  'p.r.n.',\n",
       "  'insomnia',\n",
       "  'benadryl',\n",
       "  'B.',\n",
       "  'Ischemia:',\n",
       "  'He',\n",
       "  'is',\n",
       "  'much',\n",
       "  'ruled',\n",
       "  'out',\n",
       "  'for',\n",
       "  'MI.',\n",
       "  'He',\n",
       "  'was',\n",
       "  'maintained',\n",
       "  'on',\n",
       "  'mg',\n",
       "  'p.o.',\n",
       "  'q.h.s.',\n",
       "  'p.r.n.',\n",
       "  'insomnia.',\n",
       "  'DISCHARGE',\n",
       "  'LAB',\n",
       "  'DATA:',\n",
       "  'Sodium',\n",
       "  'potassium',\n",
       "  'chloride',\n",
       "  'bicarb',\n",
       "  'BUN',\n",
       "  'creatinine',\n",
       "  'glucose',\n",
       "  'calcium',\n",
       "  'magnesium',\n",
       "  'White',\n",
       "  'blood',\n",
       "  'cell',\n",
       "  'count',\n",
       "  'hematocrit',\n",
       "  'platelets',\n",
       "  'PT',\n",
       "  'INR',\n",
       "  'PTT',\n",
       "  'DISPOSITION:',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'is',\n",
       "  'discharged',\n",
       "  'in',\n",
       "  'stable',\n",
       "  'condition',\n",
       "  'to',\n",
       "  'Grayang',\n",
       "  'Joater',\n",
       "  'Long',\n",
       "  'Community',\n",
       "  'Medical',\n",
       "  'Center',\n",
       "  'in',\n",
       "  'Hamp',\n",
       "  'Dictated',\n",
       "  'By:',\n",
       "  'JAY',\n",
       "  'WARNS',\n",
       "  'M.D.',\n",
       "  'Attending:',\n",
       "  'IRVING',\n",
       "  'B.',\n",
       "  'GIACHERIO',\n",
       "  'M.D.',\n",
       "  'Batch:',\n",
       "  'Index',\n",
       "  'No.',\n",
       "  'D:',\n",
       "  'aspirin',\n",
       "  'statin',\n",
       "  'and',\n",
       "  'T:',\n",
       "  'CC:',\n",
       "  'VALENTIN',\n",
       "  'H.',\n",
       "  'STIEG',\n",
       "  'PORTEHA',\n",
       "  'HOSPITAL',\n",
       "  'OF',\n",
       "  'CARDIOLOGY',\n",
       "  'TELEPHONE',\n",
       "  'CHESTER',\n",
       "  'VANSISE',\n",
       "  'RECORD',\n",
       "  'DH',\n",
       "  'AM',\n",
       "  'Discharge',\n",
       "  'Summary',\n",
       "  'Unsigned',\n",
       "  'DIS',\n",
       "  'Admission',\n",
       "  'Date:',\n",
       "  'Report',\n",
       "  'Status:',\n",
       "  'Unsigned',\n",
       "  'Discharge',\n",
       "  'Date:',\n",
       "  'HISTORY',\n",
       "  'OF',\n",
       "  'PRESENT',\n",
       "  'ILLNESS:',\n",
       "  'This',\n",
       "  'is',\n",
       "  'year',\n",
       "  'old',\n",
       "  'female',\n",
       "  'who',\n",
       "  'has',\n",
       "  'had',\n",
       "  'pain',\n",
       "  'in',\n",
       "  'her',\n",
       "  'old',\n",
       "  'appendectomy',\n",
       "  'incision.',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'has',\n",
       "  'had',\n",
       "  'pain',\n",
       "  'intermittently',\n",
       "  'in',\n",
       "  'the',\n",
       "  'right',\n",
       "  'C.',\n",
       "  'Pump:',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'volume',\n",
       "  'overloaded',\n",
       "  'on',\n",
       "  'admission',\n",
       "  'and',\n",
       "  'lower',\n",
       "  'quadrant',\n",
       "  'in',\n",
       "  'the',\n",
       "  'old',\n",
       "  'appendectomy',\n",
       "  'scar',\n",
       "  'for',\n",
       "  'many',\n",
       "  'years.',\n",
       "  'She',\n",
       "  'denies',\n",
       "  'nausea',\n",
       "  'or',\n",
       "  'vomiting',\n",
       "  'or',\n",
       "  'history',\n",
       "  'of',\n",
       "  'intestinal',\n",
       "  'obstruction.',\n",
       "  'PAST',\n",
       "  'MEDICAL',\n",
       "  'HISTORY:',\n",
       "  'Is',\n",
       "  'significant',\n",
       "  'for',\n",
       "  'hypertension.',\n",
       "  'She',\n",
       "  'had',\n",
       "  'an',\n",
       "  'appendectomy',\n",
       "  'in',\n",
       "  'She',\n",
       "  'had',\n",
       "  'right',\n",
       "  'carpal',\n",
       "  'tunnel',\n",
       "  'release',\n",
       "  'in',\n",
       "  'and',\n",
       "  'right',\n",
       "  'wrist',\n",
       "  'tendon',\n",
       "  'surgery',\n",
       "  'in',\n",
       "  'She',\n",
       "  'has',\n",
       "  'chronic',\n",
       "  'obstructive',\n",
       "  'pulmonary',\n",
       "  'disease.',\n",
       "  'MEDICATIONS',\n",
       "  'ON',\n",
       "  'ADMISSION:',\n",
       "  'Hydrochlorothiazide',\n",
       "  'and',\n",
       "  'Seldane',\n",
       "  'p.r.n.',\n",
       "  'for',\n",
       "  'allergies.',\n",
       "  'ALLERGIES:',\n",
       "  'PENICILLIN',\n",
       "  'CAUSES',\n",
       "  'RASH',\n",
       "  'MORPHINE',\n",
       "  'CAUSES',\n",
       "  'NAUSEA',\n",
       "  'AND',\n",
       "  'VOMITING.',\n",
       "  'She',\n",
       "  'does',\n",
       "  'not',\n",
       "  'smoke',\n",
       "  'and',\n",
       "  'only',\n",
       "  'occasionally',\n",
       "  'drinks.',\n",
       "  'REVIEW',\n",
       "  'OF',\n",
       "  'SYSTEMS:',\n",
       "  'Is',\n",
       "  'noncontributory.',\n",
       "  'CPAP',\n",
       "  'machine',\n",
       "  'was',\n",
       "  'brought',\n",
       "  'to',\n",
       "  'the',\n",
       "  'improved',\n",
       "  'after',\n",
       "  'significant',\n",
       "  'volume',\n",
       "  'reduction',\n",
       "  'by',\n",
       "  'hemodialysis.',\n",
       "  'PHYSICAL',\n",
       "  'EXAMINATION:',\n",
       "  'Blood',\n",
       "  'pressure',\n",
       "  'is',\n",
       "  'heart',\n",
       "  'rate',\n",
       "  'is',\n",
       "  'In',\n",
       "  'general',\n",
       "  'this',\n",
       "  'is',\n",
       "  'an',\n",
       "  'overweight',\n",
       "  'otherwise',\n",
       "  'well',\n",
       "  'female.',\n",
       "  'The',\n",
       "  'lungs',\n",
       "  'were',\n",
       "  'clear',\n",
       "  'to',\n",
       "  'auscultation.',\n",
       "  'Heart',\n",
       "  'was',\n",
       "  'regular',\n",
       "  'rate',\n",
       "  'and',\n",
       "  'rhythm',\n",
       "  'with',\n",
       "  'no',\n",
       "  'murmurs',\n",
       "  'rubs',\n",
       "  'or',\n",
       "  'gallops.',\n",
       "  'Pulses',\n",
       "  'were',\n",
       "  'in',\n",
       "  'the',\n",
       "  'radial',\n",
       "  'and',\n",
       "  'dorsalis',\n",
       "  'pedis.',\n",
       "  'Abdomen:',\n",
       "  'Soft',\n",
       "  'obese',\n",
       "  'nontender',\n",
       "  'with',\n",
       "  'some',\n",
       "  'mild',\n",
       "  'incisional',\n",
       "  'tenderness',\n",
       "  'over',\n",
       "  'the',\n",
       "  'area',\n",
       "  'of',\n",
       "  'the',\n",
       "  'appendectomy',\n",
       "  'scar.',\n",
       "  'Neurological',\n",
       "  'examination',\n",
       "  'was',\n",
       "  'nonfocal.',\n",
       "  'ASSESSMENT:',\n",
       "  'Was',\n",
       "  'year',\n",
       "  'old',\n",
       "  'female',\n",
       "  'with',\n",
       "  'pain',\n",
       "  'in',\n",
       "  'the',\n",
       "  'old',\n",
       "  'appendectomy',\n",
       "  'scar.',\n",
       "  'The',\n",
       "  'plan',\n",
       "  'was',\n",
       "  'for',\n",
       "  'an',\n",
       "  'incisional',\n",
       "  'hernia',\n",
       "  'repair',\n",
       "  'and',\n",
       "  'exploration.',\n",
       "  'review',\n",
       "  'of',\n",
       "  'the',\n",
       "  \"patient's\",\n",
       "  'prior',\n",
       "  'cardiac',\n",
       "  'echoes',\n",
       "  'showed',\n",
       "  'that',\n",
       "  'in',\n",
       "  'HOSPITAL',\n",
       "  'COURSE:',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'taken',\n",
       "  'to',\n",
       "  'the',\n",
       "  'operating',\n",
       "  'room',\n",
       "  'on',\n",
       "  'The',\n",
       "  'procedure',\n",
       "  'performed',\n",
       "  'was',\n",
       "  'an',\n",
       "  'incisional',\n",
       "  'hernia',\n",
       "  'repair',\n",
       "  'by',\n",
       "  'Dr.',\n",
       "  'Cartmill',\n",
       "  'and',\n",
       "  'Dr.',\n",
       "  'Borza',\n",
       "  'The',\n",
       "  'patient',\n",
       "  'tolerated',\n",
       "  'the',\n",
       "  'procedure',\n",
       "  'well',\n",
       "  'and',\n",
       "  'was',\n",
       "  'returned',\n",
       "  'to',\n",
       "  'the',\n",
       "  'floor',\n",
       "  'that',\n",
       "  'evening.',\n",
       "  'Since',\n",
       "  'that',\n",
       "  'time',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'has',\n",
       "  'had',\n",
       "  'been',\n",
       "  'somewhat',\n",
       "  'slow',\n",
       "  'to'],\n",
       " 'Tag': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " '__index_level_0__': 606,\n",
       " 'input_ids': [101,\n",
       "  22106,\n",
       "  1113,\n",
       "  1108,\n",
       "  4366,\n",
       "  1111,\n",
       "  23563,\n",
       "  6354,\n",
       "  11366,\n",
       "  6602,\n",
       "  119,\n",
       "  1272,\n",
       "  1104,\n",
       "  1231,\n",
       "  21754,\n",
       "  188,\n",
       "  5208,\n",
       "  11776,\n",
       "  1103,\n",
       "  5351,\n",
       "  1110,\n",
       "  3693,\n",
       "  1106,\n",
       "  1129,\n",
       "  3175,\n",
       "  1106,\n",
       "  189,\n",
       "  1465,\n",
       "  2332,\n",
       "  1104,\n",
       "  1107,\n",
       "  22572,\n",
       "  1162,\n",
       "  175,\n",
       "  4164,\n",
       "  1113,\n",
       "  1111,\n",
       "  187,\n",
       "  12809,\n",
       "  4060,\n",
       "  1183,\n",
       "  16430,\n",
       "  16055,\n",
       "  188,\n",
       "  5208,\n",
       "  2145,\n",
       "  119,\n",
       "  23123,\n",
       "  10024,\n",
       "  6360,\n",
       "  131,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  4441,\n",
       "  1113,\n",
       "  1107,\n",
       "  4487,\n",
       "  7912,\n",
       "  2285,\n",
       "  8362,\n",
       "  27476,\n",
       "  5796,\n",
       "  2913,\n",
       "  1119,\n",
       "  17482,\n",
       "  1394,\n",
       "  1114,\n",
       "  2273,\n",
       "  185,\n",
       "  1204,\n",
       "  17693,\n",
       "  6834,\n",
       "  1548,\n",
       "  1552,\n",
       "  119,\n",
       "  1104,\n",
       "  1123,\n",
       "  185,\n",
       "  3069,\n",
       "  1113,\n",
       "  1103,\n",
       "  1285,\n",
       "  1104,\n",
       "  12398,\n",
       "  1108,\n",
       "  1107,\n",
       "  20340,\n",
       "  2079,\n",
       "  119,\n",
       "  1231,\n",
       "  7050,\n",
       "  131,\n",
       "  1103,\n",
       "  1231,\n",
       "  7050,\n",
       "  3053,\n",
       "  1108,\n",
       "  6111,\n",
       "  2112,\n",
       "  5855,\n",
       "  4638,\n",
       "  2083,\n",
       "  2734,\n",
       "  119,\n",
       "  3463,\n",
       "  131,\n",
       "  1103,\n",
       "  5351,\n",
       "  1110,\n",
       "  1554,\n",
       "  3463,\n",
       "  119,\n",
       "  25622,\n",
       "  131,\n",
       "  1103,\n",
       "  5351,\n",
       "  1110,\n",
       "  2919,\n",
       "  1106,\n",
       "  13971,\n",
       "  4036,\n",
       "  1106,\n",
       "  2555,\n",
       "  1162,\n",
       "  2332,\n",
       "  1107,\n",
       "  1940,\n",
       "  1111,\n",
       "  187,\n",
       "  12809,\n",
       "  4060,\n",
       "  1183,\n",
       "  16430,\n",
       "  16055,\n",
       "  188,\n",
       "  5208,\n",
       "  2145,\n",
       "  119,\n",
       "  1103,\n",
       "  4036,\n",
       "  1209,\n",
       "  1321,\n",
       "  1282,\n",
       "  1118,\n",
       "  13342,\n",
       "  1113,\n",
       "  12398,\n",
       "  23897,\n",
       "  131,\n",
       "  8362,\n",
       "  27476,\n",
       "  5796,\n",
       "  2913,\n",
       "  1119,\n",
       "  17482,\n",
       "  1394,\n",
       "  1120,\n",
       "  2338,\n",
       "  1679,\n",
       "  2396,\n",
       "  1107,\n",
       "  4487,\n",
       "  7912,\n",
       "  2285,\n",
       "  189,\n",
       "  2118,\n",
       "  1120,\n",
       "  182,\n",
       "  1665,\n",
       "  1403,\n",
       "  1679,\n",
       "  2517,\n",
       "  3873,\n",
       "  1596,\n",
       "  16055,\n",
       "  1112,\n",
       "  8508,\n",
       "  4854,\n",
       "  17713,\n",
       "  186,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  8756,\n",
       "  2728,\n",
       "  2858,\n",
       "  1233,\n",
       "  17713,\n",
       "  186,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  1884,\n",
       "  17510,\n",
       "  17713,\n",
       "  171,\n",
       "  119,\n",
       "  178,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  195,\n",
       "  2556,\n",
       "  13217,\n",
       "  17713,\n",
       "  186,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  195,\n",
       "  13335,\n",
       "  1766,\n",
       "  17713,\n",
       "  185,\n",
       "  119,\n",
       "  184,\n",
       "  119,\n",
       "  186,\n",
       "  119,\n",
       "  177,\n",
       "  119,\n",
       "  188,\n",
       "  119,\n",
       "  185,\n",
       "  9516,\n",
       "  7231,\n",
       "  17713,\n",
       "  186,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  14516,\n",
       "  17031,\n",
       "  17713,\n",
       "  185,\n",
       "  119,\n",
       "  184,\n",
       "  119,\n",
       "  186,\n",
       "  119,\n",
       "  177,\n",
       "  119,\n",
       "  188,\n",
       "  119,\n",
       "  185,\n",
       "  119,\n",
       "  187,\n",
       "  119,\n",
       "  183,\n",
       "  119,\n",
       "  22233,\n",
       "  4165,\n",
       "  5813,\n",
       "  26181,\n",
       "  3556,\n",
       "  19944,\n",
       "  171,\n",
       "  119,\n",
       "  1110,\n",
       "  4386,\n",
       "  8191,\n",
       "  131,\n",
       "  1119,\n",
       "  1110,\n",
       "  1277,\n",
       "  4741,\n",
       "  1149,\n",
       "  1111,\n",
       "  1940,\n",
       "  119,\n",
       "  1119,\n",
       "  1108,\n",
       "  4441,\n",
       "  1113,\n",
       "  17713,\n",
       "  185,\n",
       "  119,\n",
       "  184,\n",
       "  119,\n",
       "  186,\n",
       "  119,\n",
       "  177,\n",
       "  119,\n",
       "  188,\n",
       "  119,\n",
       "  185,\n",
       "  119,\n",
       "  187,\n",
       "  119,\n",
       "  183,\n",
       "  119,\n",
       "  22233,\n",
       "  4165,\n",
       "  5813,\n",
       "  119,\n",
       "  12398,\n",
       "  8074,\n",
       "  2233,\n",
       "  131,\n",
       "  15059,\n",
       "  21177,\n",
       "  21256,\n",
       "  16516,\n",
       "  8766,\n",
       "  1830,\n",
       "  171,\n",
       "  3488,\n",
       "  172,\n",
       "  11811,\n",
       "  6105,\n",
       "  2042,\n",
       "  20636,\n",
       "  15355,\n",
       "  12477,\n",
       "  27844,\n",
       "  1653,\n",
       "  1892,\n",
       "  2765,\n",
       "  5099,\n",
       "  23123,\n",
       "  10024,\n",
       "  1665,\n",
       "  7729,\n",
       "  4885,\n",
       "  9585,\n",
       "  185,\n",
       "  1204,\n",
       "  1107,\n",
       "  1197,\n",
       "  185,\n",
       "  3069,\n",
       "  25622,\n",
       "  131,\n",
       "  1103,\n",
       "  5351,\n",
       "  1110,\n",
       "  15207,\n",
       "  1107,\n",
       "  6111,\n",
       "  3879,\n",
       "  1106,\n",
       "  5021,\n",
       "  4993,\n",
       "  179,\n",
       "  20534,\n",
       "  1200,\n",
       "  1263,\n",
       "  1661,\n",
       "  2657,\n",
       "  2057,\n",
       "  1107,\n",
       "  5871,\n",
       "  8223,\n",
       "  26754,\n",
       "  1118,\n",
       "  131,\n",
       "  179,\n",
       "  4164,\n",
       "  21310,\n",
       "  182,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  6546,\n",
       "  131,\n",
       "  178,\n",
       "  22466,\n",
       "  1403,\n",
       "  171,\n",
       "  119,\n",
       "  176,\n",
       "  12571,\n",
       "  4679,\n",
       "  2660,\n",
       "  182,\n",
       "  119,\n",
       "  173,\n",
       "  119,\n",
       "  15817,\n",
       "  131,\n",
       "  7448,\n",
       "  1185,\n",
       "  119,\n",
       "  173,\n",
       "  131,\n",
       "  1112,\n",
       "  8508,\n",
       "  4854,\n",
       "  188,\n",
       "  19756,\n",
       "  1394,\n",
       "  1105,\n",
       "  189,\n",
       "  131,\n",
       "  14402,\n",
       "  131,\n",
       "  191,\n",
       "  7531,\n",
       "  26728,\n",
       "  177,\n",
       "  119,\n",
       "  188,\n",
       "  9570,\n",
       "  1403,\n",
       "  4104,\n",
       "  10486,\n",
       "  1161,\n",
       "  2704,\n",
       "  1104,\n",
       "  3621,\n",
       "  17288,\n",
       "  7314,\n",
       "  2229,\n",
       "  1200,\n",
       "  3498,\n",
       "  4863,\n",
       "  1162,\n",
       "  1647,\n",
       "  173,\n",
       "  1324,\n",
       "  1821,\n",
       "  12398,\n",
       "  14940,\n",
       "  8362,\n",
       "  16173,\n",
       "  4267,\n",
       "  1116,\n",
       "  10296,\n",
       "  2236,\n",
       "  131,\n",
       "  2592,\n",
       "  2781,\n",
       "  131,\n",
       "  8362,\n",
       "  16173,\n",
       "  12398,\n",
       "  2236,\n",
       "  131,\n",
       "  1607,\n",
       "  1104,\n",
       "  1675,\n",
       "  6946,\n",
       "  131,\n",
       "  1142,\n",
       "  1110,\n",
       "  1214,\n",
       "  1385,\n",
       "  2130,\n",
       "  1150,\n",
       "  1144,\n",
       "  1125,\n",
       "  2489,\n",
       "  1107,\n",
       "  1123,\n",
       "  1385,\n",
       "  12647,\n",
       "  6696,\n",
       "  10294,\n",
       "  18778,\n",
       "  1183,\n",
       "  1107,\n",
       "  16073,\n",
       "  119,\n",
       "  1103,\n",
       "  5351,\n",
       "  1144,\n",
       "  1125,\n",
       "  2489,\n",
       "  27946,\n",
       "  1193,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  172,\n",
       "  119,\n",
       "  11188,\n",
       "  131,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  3884,\n",
       "  1166,\n",
       "  18268,\n",
       "  1113,\n",
       "  10296,\n",
       "  1105,\n",
       "  2211,\n",
       "  186,\n",
       "  18413,\n",
       "  6922,\n",
       "  1107,\n",
       "  1103,\n",
       "  1385,\n",
       "  12647,\n",
       "  6696,\n",
       "  10294,\n",
       "  18778,\n",
       "  1183,\n",
       "  14161,\n",
       "  1111,\n",
       "  1242,\n",
       "  1201,\n",
       "  119,\n",
       "  1131,\n",
       "  26360,\n",
       "  22882,\n",
       "  1137,\n",
       "  26979,\n",
       "  1158,\n",
       "  1137,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_train = tokenized_data_train.remove_columns(['Stride'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['Stride'])\n",
    "\n",
    "tokenized_data_train = tokenized_data_train.remove_columns(['Word'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['Word'])\n",
    "\n",
    "tokenized_data_train = tokenized_data_train.remove_columns(['Tag'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['Tag'])\n",
    "\n",
    "tokenized_data_train = tokenized_data_train.remove_columns(['__index_level_0__'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding = 'max_length',  max_length =512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at C:\\Users\\jonat/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/pytorch_model.bin from cache at C:\\Users\\jonat/.cache\\huggingface\\transformers\\794538e7c825dc7be96d9fc3c73b79a9736da5f699fc50d31513dbca0740b349.f0d8b668347b3048f5b88e273fde3c3412366726bc99aa5935b7990944092fb1\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForTokenClassification.from_pretrained(BERT_MODEL, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    \n",
    "    all_pred = [p for ps in true_predictions for p in ps]\n",
    "    all_label = [l for ls in true_labels for l in ls]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_label, all_pred, average='binary')\n",
    "    acc = accuracy_score(all_label, all_pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./epochs',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-7,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps= 2000,\n",
    "    eval_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1661\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 41500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25031' max='41500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25031/41500 4:35:05 < 3:01:00, 1.52 it/s, Epoch 60.31/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.081191</td>\n",
       "      <td>0.991197</td>\n",
       "      <td>0.816992</td>\n",
       "      <td>0.815732</td>\n",
       "      <td>0.818255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.081031</td>\n",
       "      <td>0.991183</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.815699</td>\n",
       "      <td>0.817568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.080843</td>\n",
       "      <td>0.991186</td>\n",
       "      <td>0.816658</td>\n",
       "      <td>0.815865</td>\n",
       "      <td>0.817453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.080867</td>\n",
       "      <td>0.991186</td>\n",
       "      <td>0.816595</td>\n",
       "      <td>0.816081</td>\n",
       "      <td>0.817109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080833</td>\n",
       "      <td>0.991197</td>\n",
       "      <td>0.816572</td>\n",
       "      <td>0.817181</td>\n",
       "      <td>0.815964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080823</td>\n",
       "      <td>0.991197</td>\n",
       "      <td>0.816677</td>\n",
       "      <td>0.816818</td>\n",
       "      <td>0.816537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080806</td>\n",
       "      <td>0.991203</td>\n",
       "      <td>0.816666</td>\n",
       "      <td>0.817368</td>\n",
       "      <td>0.815964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>0.816703</td>\n",
       "      <td>0.816984</td>\n",
       "      <td>0.816422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080608</td>\n",
       "      <td>0.991197</td>\n",
       "      <td>0.816614</td>\n",
       "      <td>0.817035</td>\n",
       "      <td>0.816193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080671</td>\n",
       "      <td>0.991197</td>\n",
       "      <td>0.816593</td>\n",
       "      <td>0.817108</td>\n",
       "      <td>0.816079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080733</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.816680</td>\n",
       "      <td>0.818088</td>\n",
       "      <td>0.815277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>0.991222</td>\n",
       "      <td>0.816867</td>\n",
       "      <td>0.818464</td>\n",
       "      <td>0.815277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080677</td>\n",
       "      <td>0.991236</td>\n",
       "      <td>0.816934</td>\n",
       "      <td>0.819523</td>\n",
       "      <td>0.814361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080696</td>\n",
       "      <td>0.991238</td>\n",
       "      <td>0.816939</td>\n",
       "      <td>0.819765</td>\n",
       "      <td>0.814132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080753</td>\n",
       "      <td>0.991238</td>\n",
       "      <td>0.816918</td>\n",
       "      <td>0.819839</td>\n",
       "      <td>0.814017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080824</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.816969</td>\n",
       "      <td>0.820175</td>\n",
       "      <td>0.813788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080716</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.817277</td>\n",
       "      <td>0.820913</td>\n",
       "      <td>0.813674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080642</td>\n",
       "      <td>0.991258</td>\n",
       "      <td>0.817057</td>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.812987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080749</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.817768</td>\n",
       "      <td>0.822841</td>\n",
       "      <td>0.812758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080734</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.817448</td>\n",
       "      <td>0.822663</td>\n",
       "      <td>0.812300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.991277</td>\n",
       "      <td>0.817049</td>\n",
       "      <td>0.823030</td>\n",
       "      <td>0.811154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080849</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.816913</td>\n",
       "      <td>0.822989</td>\n",
       "      <td>0.810925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080885</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.816913</td>\n",
       "      <td>0.822989</td>\n",
       "      <td>0.810925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080915</td>\n",
       "      <td>0.991274</td>\n",
       "      <td>0.816918</td>\n",
       "      <td>0.823235</td>\n",
       "      <td>0.810696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080958</td>\n",
       "      <td>0.991280</td>\n",
       "      <td>0.816927</td>\n",
       "      <td>0.823728</td>\n",
       "      <td>0.810238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081014</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.816355</td>\n",
       "      <td>0.823392</td>\n",
       "      <td>0.809437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081084</td>\n",
       "      <td>0.991247</td>\n",
       "      <td>0.816086</td>\n",
       "      <td>0.823557</td>\n",
       "      <td>0.808749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081089</td>\n",
       "      <td>0.991241</td>\n",
       "      <td>0.815843</td>\n",
       "      <td>0.823893</td>\n",
       "      <td>0.807948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081137</td>\n",
       "      <td>0.991236</td>\n",
       "      <td>0.815620</td>\n",
       "      <td>0.824155</td>\n",
       "      <td>0.807261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081058</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814858</td>\n",
       "      <td>0.824751</td>\n",
       "      <td>0.805199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081101</td>\n",
       "      <td>0.991227</td>\n",
       "      <td>0.814987</td>\n",
       "      <td>0.825617</td>\n",
       "      <td>0.804627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081293</td>\n",
       "      <td>0.991222</td>\n",
       "      <td>0.814806</td>\n",
       "      <td>0.825729</td>\n",
       "      <td>0.804169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081278</td>\n",
       "      <td>0.991222</td>\n",
       "      <td>0.814849</td>\n",
       "      <td>0.825576</td>\n",
       "      <td>0.804398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081232</td>\n",
       "      <td>0.991222</td>\n",
       "      <td>0.814935</td>\n",
       "      <td>0.825270</td>\n",
       "      <td>0.804856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081096</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814428</td>\n",
       "      <td>0.826282</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080981</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814492</td>\n",
       "      <td>0.826051</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081031</td>\n",
       "      <td>0.991216</td>\n",
       "      <td>0.814540</td>\n",
       "      <td>0.826148</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>0.991219</td>\n",
       "      <td>0.814608</td>\n",
       "      <td>0.826169</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080692</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>0.814385</td>\n",
       "      <td>0.825106</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080654</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>0.814321</td>\n",
       "      <td>0.825335</td>\n",
       "      <td>0.803596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080684</td>\n",
       "      <td>0.991192</td>\n",
       "      <td>0.814071</td>\n",
       "      <td>0.825427</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080632</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.814510</td>\n",
       "      <td>0.825724</td>\n",
       "      <td>0.803596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080660</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814600</td>\n",
       "      <td>0.825668</td>\n",
       "      <td>0.803825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080567</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814621</td>\n",
       "      <td>0.825591</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080610</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814621</td>\n",
       "      <td>0.825591</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080673</td>\n",
       "      <td>0.991216</td>\n",
       "      <td>0.814669</td>\n",
       "      <td>0.825688</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080718</td>\n",
       "      <td>0.991222</td>\n",
       "      <td>0.814720</td>\n",
       "      <td>0.826036</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080376</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>0.814664</td>\n",
       "      <td>0.825438</td>\n",
       "      <td>0.804169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.991216</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>0.804398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.991238</td>\n",
       "      <td>0.815304</td>\n",
       "      <td>0.825546</td>\n",
       "      <td>0.805314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080469</td>\n",
       "      <td>0.991241</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.824804</td>\n",
       "      <td>0.806574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080362</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.816336</td>\n",
       "      <td>0.822408</td>\n",
       "      <td>0.810353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080376</td>\n",
       "      <td>0.991252</td>\n",
       "      <td>0.816286</td>\n",
       "      <td>0.823372</td>\n",
       "      <td>0.809322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080394</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.816249</td>\n",
       "      <td>0.823770</td>\n",
       "      <td>0.808864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.080303</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.816164</td>\n",
       "      <td>0.824072</td>\n",
       "      <td>0.808406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.080202</td>\n",
       "      <td>0.991258</td>\n",
       "      <td>0.816296</td>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.808864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.080254</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.824037</td>\n",
       "      <td>0.808749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.080140</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.816029</td>\n",
       "      <td>0.825348</td>\n",
       "      <td>0.806917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.080156</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.816135</td>\n",
       "      <td>0.824968</td>\n",
       "      <td>0.807490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080223</td>\n",
       "      <td>0.991258</td>\n",
       "      <td>0.816019</td>\n",
       "      <td>0.824851</td>\n",
       "      <td>0.807375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080265</td>\n",
       "      <td>0.991258</td>\n",
       "      <td>0.815977</td>\n",
       "      <td>0.825003</td>\n",
       "      <td>0.807146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080305</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.816071</td>\n",
       "      <td>0.825196</td>\n",
       "      <td>0.807146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080387</td>\n",
       "      <td>0.991249</td>\n",
       "      <td>0.815793</td>\n",
       "      <td>0.824865</td>\n",
       "      <td>0.806917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080412</td>\n",
       "      <td>0.991241</td>\n",
       "      <td>0.815630</td>\n",
       "      <td>0.824652</td>\n",
       "      <td>0.806803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080444</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.815762</td>\n",
       "      <td>0.824444</td>\n",
       "      <td>0.807261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080370</td>\n",
       "      <td>0.991241</td>\n",
       "      <td>0.815694</td>\n",
       "      <td>0.824424</td>\n",
       "      <td>0.807146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.991258</td>\n",
       "      <td>0.816147</td>\n",
       "      <td>0.824395</td>\n",
       "      <td>0.808062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080391</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.816718</td>\n",
       "      <td>0.823420</td>\n",
       "      <td>0.810124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080334</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.816607</td>\n",
       "      <td>0.823550</td>\n",
       "      <td>0.809780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080266</td>\n",
       "      <td>0.991274</td>\n",
       "      <td>0.816791</td>\n",
       "      <td>0.823687</td>\n",
       "      <td>0.810009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080159</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.816334</td>\n",
       "      <td>0.823468</td>\n",
       "      <td>0.809322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080206</td>\n",
       "      <td>0.991252</td>\n",
       "      <td>0.816265</td>\n",
       "      <td>0.823447</td>\n",
       "      <td>0.809208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080111</td>\n",
       "      <td>0.991252</td>\n",
       "      <td>0.816117</td>\n",
       "      <td>0.823976</td>\n",
       "      <td>0.808406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080164</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.816206</td>\n",
       "      <td>0.823921</td>\n",
       "      <td>0.808635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080223</td>\n",
       "      <td>0.991249</td>\n",
       "      <td>0.816027</td>\n",
       "      <td>0.824031</td>\n",
       "      <td>0.808177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.815879</td>\n",
       "      <td>0.825883</td>\n",
       "      <td>0.806115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080206</td>\n",
       "      <td>0.991258</td>\n",
       "      <td>0.815742</td>\n",
       "      <td>0.825842</td>\n",
       "      <td>0.805886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080224</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.815588</td>\n",
       "      <td>0.826128</td>\n",
       "      <td>0.805314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080271</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.815567</td>\n",
       "      <td>0.826204</td>\n",
       "      <td>0.805199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080305</td>\n",
       "      <td>0.991249</td>\n",
       "      <td>0.815451</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.805085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080244</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.815623</td>\n",
       "      <td>0.826803</td>\n",
       "      <td>0.804741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815718</td>\n",
       "      <td>0.826998</td>\n",
       "      <td>0.804741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079983</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.827050</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080010</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.827050</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080017</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.815383</td>\n",
       "      <td>0.827399</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080058</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.815383</td>\n",
       "      <td>0.827399</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080134</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.815318</td>\n",
       "      <td>0.827631</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080130</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.815456</td>\n",
       "      <td>0.827672</td>\n",
       "      <td>0.803596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080103</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.815297</td>\n",
       "      <td>0.827708</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080118</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815418</td>\n",
       "      <td>0.828079</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080168</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815418</td>\n",
       "      <td>0.828079</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080204</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815418</td>\n",
       "      <td>0.828079</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080247</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.815370</td>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080307</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.815392</td>\n",
       "      <td>0.827904</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080103</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.815478</td>\n",
       "      <td>0.827594</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080194</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815353</td>\n",
       "      <td>0.828311</td>\n",
       "      <td>0.802794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080243</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.815237</td>\n",
       "      <td>0.828193</td>\n",
       "      <td>0.802680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080280</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.815387</td>\n",
       "      <td>0.827651</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080379</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.815392</td>\n",
       "      <td>0.827904</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080444</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.815366</td>\n",
       "      <td>0.827729</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080481</td>\n",
       "      <td>0.991263</td>\n",
       "      <td>0.815366</td>\n",
       "      <td>0.827729</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080471</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815461</td>\n",
       "      <td>0.827924</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080506</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.815461</td>\n",
       "      <td>0.827924</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080552</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.815198</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080639</td>\n",
       "      <td>0.991255</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.828910</td>\n",
       "      <td>0.801191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080611</td>\n",
       "      <td>0.991252</td>\n",
       "      <td>0.814703</td>\n",
       "      <td>0.829046</td>\n",
       "      <td>0.800847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080697</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.814431</td>\n",
       "      <td>0.829219</td>\n",
       "      <td>0.800160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080734</td>\n",
       "      <td>0.991247</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.829239</td>\n",
       "      <td>0.800275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080655</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.815358</td>\n",
       "      <td>0.828565</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080591</td>\n",
       "      <td>0.991277</td>\n",
       "      <td>0.815538</td>\n",
       "      <td>0.828450</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080625</td>\n",
       "      <td>0.991277</td>\n",
       "      <td>0.815538</td>\n",
       "      <td>0.828450</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080663</td>\n",
       "      <td>0.991280</td>\n",
       "      <td>0.815607</td>\n",
       "      <td>0.828470</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.991280</td>\n",
       "      <td>0.815607</td>\n",
       "      <td>0.828470</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080656</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.815125</td>\n",
       "      <td>0.828328</td>\n",
       "      <td>0.802336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080704</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.816123</td>\n",
       "      <td>0.827952</td>\n",
       "      <td>0.804627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080743</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.816123</td>\n",
       "      <td>0.827952</td>\n",
       "      <td>0.804627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0.828086</td>\n",
       "      <td>0.804283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080705</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.815921</td>\n",
       "      <td>0.828143</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080782</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.816059</td>\n",
       "      <td>0.828184</td>\n",
       "      <td>0.804283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080839</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.815942</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.804169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080877</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.815921</td>\n",
       "      <td>0.828143</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080908</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.815921</td>\n",
       "      <td>0.828143</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.815849</td>\n",
       "      <td>0.829213</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080998</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815854</td>\n",
       "      <td>0.829467</td>\n",
       "      <td>0.802680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080993</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815716</td>\n",
       "      <td>0.829427</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080978</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815690</td>\n",
       "      <td>0.829251</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080954</td>\n",
       "      <td>0.991285</td>\n",
       "      <td>0.815702</td>\n",
       "      <td>0.828666</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081027</td>\n",
       "      <td>0.991282</td>\n",
       "      <td>0.815741</td>\n",
       "      <td>0.828258</td>\n",
       "      <td>0.803596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815861</td>\n",
       "      <td>0.828629</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815861</td>\n",
       "      <td>0.828629</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081152</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815861</td>\n",
       "      <td>0.828629</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081203</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815861</td>\n",
       "      <td>0.828629</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081276</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815840</td>\n",
       "      <td>0.828706</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081310</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815840</td>\n",
       "      <td>0.828706</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081403</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.815750</td>\n",
       "      <td>0.828764</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081241</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815840</td>\n",
       "      <td>0.828706</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081345</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.816115</td>\n",
       "      <td>0.828787</td>\n",
       "      <td>0.803825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081390</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.816184</td>\n",
       "      <td>0.828808</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081451</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816279</td>\n",
       "      <td>0.829003</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081507</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.815999</td>\n",
       "      <td>0.828669</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081570</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815887</td>\n",
       "      <td>0.828804</td>\n",
       "      <td>0.803367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081599</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081614</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816001</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081648</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815953</td>\n",
       "      <td>0.829918</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081695</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815953</td>\n",
       "      <td>0.829918</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816001</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081801</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816001</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081729</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816022</td>\n",
       "      <td>0.829938</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816022</td>\n",
       "      <td>0.829938</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081704</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.815579</td>\n",
       "      <td>0.830740</td>\n",
       "      <td>0.800962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081736</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.815579</td>\n",
       "      <td>0.830740</td>\n",
       "      <td>0.800962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081772</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.815579</td>\n",
       "      <td>0.830740</td>\n",
       "      <td>0.800962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081764</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815975</td>\n",
       "      <td>0.829840</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081677</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815704</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.801878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081695</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.829994</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081721</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815704</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.801878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081702</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.816018</td>\n",
       "      <td>0.829684</td>\n",
       "      <td>0.802794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081676</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.816070</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081705</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815932</td>\n",
       "      <td>0.829996</td>\n",
       "      <td>0.802336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081805</td>\n",
       "      <td>0.991337</td>\n",
       "      <td>0.816669</td>\n",
       "      <td>0.830296</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081822</td>\n",
       "      <td>0.991337</td>\n",
       "      <td>0.816669</td>\n",
       "      <td>0.830296</td>\n",
       "      <td>0.803481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081811</td>\n",
       "      <td>0.991337</td>\n",
       "      <td>0.816690</td>\n",
       "      <td>0.830218</td>\n",
       "      <td>0.803596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081849</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.816759</td>\n",
       "      <td>0.830238</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081801</td>\n",
       "      <td>0.991329</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.830157</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081823</td>\n",
       "      <td>0.991329</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.830157</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081862</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.816823</td>\n",
       "      <td>0.830004</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081889</td>\n",
       "      <td>0.991329</td>\n",
       "      <td>0.816675</td>\n",
       "      <td>0.829456</td>\n",
       "      <td>0.804283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081916</td>\n",
       "      <td>0.991337</td>\n",
       "      <td>0.816818</td>\n",
       "      <td>0.829750</td>\n",
       "      <td>0.804283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081948</td>\n",
       "      <td>0.991337</td>\n",
       "      <td>0.816839</td>\n",
       "      <td>0.829672</td>\n",
       "      <td>0.804398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081988</td>\n",
       "      <td>0.991335</td>\n",
       "      <td>0.816749</td>\n",
       "      <td>0.829729</td>\n",
       "      <td>0.804169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082003</td>\n",
       "      <td>0.991329</td>\n",
       "      <td>0.816611</td>\n",
       "      <td>0.829689</td>\n",
       "      <td>0.803940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082059</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.816759</td>\n",
       "      <td>0.830238</td>\n",
       "      <td>0.803710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082061</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815713</td>\n",
       "      <td>0.830525</td>\n",
       "      <td>0.801420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082098</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815829</td>\n",
       "      <td>0.830643</td>\n",
       "      <td>0.801535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082126</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815829</td>\n",
       "      <td>0.830643</td>\n",
       "      <td>0.801535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082151</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815894</td>\n",
       "      <td>0.830408</td>\n",
       "      <td>0.801878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815894</td>\n",
       "      <td>0.830408</td>\n",
       "      <td>0.801878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082229</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815825</td>\n",
       "      <td>0.830388</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082267</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.815549</td>\n",
       "      <td>0.830307</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082172</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815799</td>\n",
       "      <td>0.830211</td>\n",
       "      <td>0.801878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082188</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.815708</td>\n",
       "      <td>0.830269</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082168</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815851</td>\n",
       "      <td>0.830565</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815851</td>\n",
       "      <td>0.830565</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>0.991318</td>\n",
       "      <td>0.815951</td>\n",
       "      <td>0.831018</td>\n",
       "      <td>0.801420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082306</td>\n",
       "      <td>0.991318</td>\n",
       "      <td>0.815951</td>\n",
       "      <td>0.831018</td>\n",
       "      <td>0.801420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082334</td>\n",
       "      <td>0.991315</td>\n",
       "      <td>0.815882</td>\n",
       "      <td>0.830998</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082360</td>\n",
       "      <td>0.991315</td>\n",
       "      <td>0.815882</td>\n",
       "      <td>0.830998</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082383</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.815812</td>\n",
       "      <td>0.830978</td>\n",
       "      <td>0.801191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082413</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815851</td>\n",
       "      <td>0.830565</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082375</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.830446</td>\n",
       "      <td>0.801535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.830446</td>\n",
       "      <td>0.801535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082215</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815975</td>\n",
       "      <td>0.829840</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082236</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816044</td>\n",
       "      <td>0.829860</td>\n",
       "      <td>0.802680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082273</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.816091</td>\n",
       "      <td>0.829959</td>\n",
       "      <td>0.802680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082302</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.816018</td>\n",
       "      <td>0.829684</td>\n",
       "      <td>0.802794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082322</td>\n",
       "      <td>0.991318</td>\n",
       "      <td>0.816251</td>\n",
       "      <td>0.829921</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082352</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.816113</td>\n",
       "      <td>0.829880</td>\n",
       "      <td>0.802794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082309</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816087</td>\n",
       "      <td>0.829704</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082332</td>\n",
       "      <td>0.991318</td>\n",
       "      <td>0.816272</td>\n",
       "      <td>0.829843</td>\n",
       "      <td>0.803138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082358</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.816134</td>\n",
       "      <td>0.829802</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082307</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082341</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.816006</td>\n",
       "      <td>0.830271</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082393</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082412</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082433</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082459</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.816027</td>\n",
       "      <td>0.830193</td>\n",
       "      <td>0.802336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082475</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>0.815889</td>\n",
       "      <td>0.830153</td>\n",
       "      <td>0.802107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082505</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815842</td>\n",
       "      <td>0.830055</td>\n",
       "      <td>0.802107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>0.991315</td>\n",
       "      <td>0.816096</td>\n",
       "      <td>0.830213</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082574</td>\n",
       "      <td>0.991315</td>\n",
       "      <td>0.816096</td>\n",
       "      <td>0.830213</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082596</td>\n",
       "      <td>0.991315</td>\n",
       "      <td>0.816096</td>\n",
       "      <td>0.830213</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082643</td>\n",
       "      <td>0.991315</td>\n",
       "      <td>0.816096</td>\n",
       "      <td>0.830213</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082678</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.815898</td>\n",
       "      <td>0.830663</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082699</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.815898</td>\n",
       "      <td>0.830663</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082717</td>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.815898</td>\n",
       "      <td>0.830663</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082710</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.830446</td>\n",
       "      <td>0.801535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082717</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815756</td>\n",
       "      <td>0.830368</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082726</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.815751</td>\n",
       "      <td>0.830113</td>\n",
       "      <td>0.801878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815682</td>\n",
       "      <td>0.830092</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082632</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082661</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815725</td>\n",
       "      <td>0.829936</td>\n",
       "      <td>0.801993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082381</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815992</td>\n",
       "      <td>0.829508</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.815992</td>\n",
       "      <td>0.829508</td>\n",
       "      <td>0.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082426</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.816013</td>\n",
       "      <td>0.829430</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082448</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.816013</td>\n",
       "      <td>0.829430</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082460</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.816013</td>\n",
       "      <td>0.829430</td>\n",
       "      <td>0.803023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082496</td>\n",
       "      <td>0.991310</td>\n",
       "      <td>0.816151</td>\n",
       "      <td>0.829470</td>\n",
       "      <td>0.803252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082380</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815626</td>\n",
       "      <td>0.829485</td>\n",
       "      <td>0.802222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082410</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.815699</td>\n",
       "      <td>0.829760</td>\n",
       "      <td>0.802107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082402</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815832</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082418</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815832</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.802565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.815901</td>\n",
       "      <td>0.829566</td>\n",
       "      <td>0.802680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082438</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.829329</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082465</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.815600</td>\n",
       "      <td>0.829309</td>\n",
       "      <td>0.802336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082535</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.815699</td>\n",
       "      <td>0.829760</td>\n",
       "      <td>0.802107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082487</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815630</td>\n",
       "      <td>0.829739</td>\n",
       "      <td>0.801993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082479</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082514</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082529</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082541</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082559</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815587</td>\n",
       "      <td>0.829896</td>\n",
       "      <td>0.801764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082589</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>0.815518</td>\n",
       "      <td>0.829876</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082615</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815566</td>\n",
       "      <td>0.829974</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082642</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815566</td>\n",
       "      <td>0.829974</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>0.991285</td>\n",
       "      <td>0.815359</td>\n",
       "      <td>0.829913</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082680</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815501</td>\n",
       "      <td>0.830209</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082704</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815501</td>\n",
       "      <td>0.830209</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082720</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.815501</td>\n",
       "      <td>0.830209</td>\n",
       "      <td>0.801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082728</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.815661</td>\n",
       "      <td>0.830171</td>\n",
       "      <td>0.801649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-2000\n",
      "Configuration saved in ./epochs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-2000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-4000\n",
      "Configuration saved in ./epochs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-6000\n",
      "Configuration saved in ./epochs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-6000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-8000\n",
      "Configuration saved in ./epochs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-8000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-10000\n",
      "Configuration saved in ./epochs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-10000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-12000\n",
      "Configuration saved in ./epochs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-12000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-14000\n",
      "Configuration saved in ./epochs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-14000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-16000\n",
      "Configuration saved in ./epochs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-16000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-18000\n",
      "Configuration saved in ./epochs\\checkpoint-18000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-18000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-20000\n",
      "Configuration saved in ./epochs\\checkpoint-20000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-22000\n",
      "Configuration saved in ./epochs\\checkpoint-22000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-22000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-24000\n",
      "Configuration saved in ./epochs\\checkpoint-24000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-24000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4008/2898178969.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1330\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1922\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1924\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1925\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1740\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1743\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 )\n\u001b[0;32m    584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     ):\n\u001b[1;32m--> 402\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\extraction\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data_train,\n",
    "    eval_dataset=tokenized_data_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
